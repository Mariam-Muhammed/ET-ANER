{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeITOkwIRbBP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Verify that you have the GPU recognized\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBLqJOI8hFmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtO91IXuujsZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers==4.45.2\n",
        "!pip install torch==2.5.1 torchvision==0.20.1\n",
        "!pip install seqeval==1.2.2\n",
        "!pip uninstall -y torchaudio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnvxnWZ_hwU8"
      },
      "outputs": [],
      "source": [
        "#######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivJDyClEatWV"
      },
      "outputs": [],
      "source": [
        "!export PYTHONPATH=PYTHONPATH:/path/to/ArabicNER"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following part of the code is adapted from:\n",
        "Mustafa Jarrar, Mohammed Khalilia, and Sana Ghanem (2022) \"Wojood: Nested Arabic Named Entity Corpus and Recognition using BERT. \"Proceedings of the International Conference on Language Resources and Evaluation (LREC 2022), Marseille, France.\n",
        "# Source: https://github.com/SinaLab/ArabicNER\n"
      ],
      "metadata": {
        "id": "SaZt6c5VknZE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLHHpnFXr7-K"
      },
      "outputs": [],
      "source": [
        "# Remove existing package and clone again from Github\n",
        "!rm -rf /content/ArabicNER\n",
        "!git clone https://github.com/SinaLab/ArabicNER.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4EvIlcrssU6"
      },
      "outputs": [],
      "source": [
        "# Add the ArabicNER package to the system path\n",
        "import sys\n",
        "import argparse\n",
        "sys.path.append('/content/ArabicNER/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rM-8p4nsztp"
      },
      "outputs": [],
      "source": [
        "# Import train function\n",
        "from arabiner.bin.train import main as train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We changed the model name each time to test different models:\n",
        "\n",
        "in this line      \n",
        "  \"kwargs\": {\"dropout\": 0.1, \"bert_model\": \"qarib/bert-base-qarib\"}  \n"
      ],
      "metadata": {
        "id": "wBluqkqzTuHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqqvY1fXtZpD"
      },
      "outputs": [],
      "source": [
        "# Setup the model arguments\n",
        "args_dict = {\n",
        "    # Model output path to save artifacts and model predictions\n",
        "    \"output_path\": \"/content/output/\",\n",
        "\n",
        "    # train/test/validation data paths\n",
        "    \"train_path\": \"/content/ANERtrainC10.txt\",\n",
        "    \"test_path\": \"/content/ANERtestC10.txt\",\n",
        "    \"val_path\": \"/content/ANERvalC10.txt\",\n",
        "\n",
        "    # seed for randomization\n",
        "    \"seed\": 1,\n",
        "\n",
        "    \"batch_size\": 16,\n",
        "\n",
        "    # Nmber of workers for the dataloader\n",
        "    \"num_workers\": 1,\n",
        "\n",
        "    # GPU/device Ids to train model on\n",
        "    # For two GPUs use [0, 1]\n",
        "    # For three GPUs use [0, 1, 2], etc.\n",
        "    \"gpus\": [0],\n",
        "\n",
        "    # Overwrite data in output_path directory specified above\n",
        "    \"overwrite\": True,\n",
        "\n",
        "    # How often to print the logs in terms of number of steps\n",
        "    \"log_interval\": 10,\n",
        "\n",
        "    # Data configuration\n",
        "    # Here we specify the dataset class and there are two options:\n",
        "    #  arabiner.data.datasets.DefaultDataset: for flat NER\n",
        "    #  arabiner.data.datasets.NestedTagsDataset: for nested NER\n",
        "    #\n",
        "    # kwargs: keyword arguments to the dataset class\n",
        "    # This notebook used the DefaultDataset for flat NER\n",
        "    \"data_config\": {\n",
        "        \"fn\": \"arabiner.data.datasets.DefaultDataset\",\n",
        "        \"kwargs\": {\"max_seq_len\": 256}\n",
        "    },\n",
        "\n",
        "    # Neural net configuration\n",
        "    # There are two NNs:\n",
        "    #   arabiner.nn.BertSeqTagger: flat NER tagger\n",
        "    #   arabiner.nn.BertNestedTagger: nested NER tagger\n",
        "    #\n",
        "    # kwargs: keyword arguments to the NN\n",
        "    # This notebook uses BertSeqTagger for flat NER tagging\n",
        "    \"network_config\": {\n",
        "        \"fn\": \"arabiner.nn.BertSeqTagger\",\n",
        "        \"kwargs\": {\"dropout\": 0.1, \"bert_model\": \"qarib/bert-base-qarib\"}  #  We changed this model name each time to test different models\n",
        "\n",
        "    },\n",
        "\n",
        "    # Model trainer configuration\n",
        "    #\n",
        "    #  arabiner.trainers.BertTrainer: for flat NER training\n",
        "    #  arabiner.trainers.BertNestedTrainer: for nested NER training\n",
        "    #\n",
        "    # kwargs: keyword arguments to arabiner.trainers.BertTrainer\n",
        "    #         additional arguments you can pass includes\n",
        "    #           - clip: for gradient clpping\n",
        "    #           - patience: number of epochs for early termination\n",
        "    # This notebook uses BertTrainer for fat NER training\n",
        "    \"trainer_config\": {\n",
        "        \"fn\": \"arabiner.trainers.BertTrainer\",\n",
        "        \"kwargs\": {\"max_epochs\": 50}\n",
        "    },\n",
        "\n",
        "    # Optimizer configuration\n",
        "    # Our experiments use torch.optim.AdamW, however, you are free to pass\n",
        "    # any other optmizers such as torch.optim.Adam or torch.optim.SGD\n",
        "    # lr: learning rate\n",
        "    # kwargs: keyword arguments to torch.optim.AdamW or whatever optimizer you use\n",
        "    #\n",
        "    # Additional optimizers can be found here:\n",
        "    # https://pytorch.org/docs/stable/optim.html\n",
        "    \"optimizer\": {\n",
        "        \"fn\": \"torch.optim.AdamW\",\n",
        "        \"kwargs\": {\"lr\": 1e-5}\n",
        "    },\n",
        "\n",
        "    # Learning rate scheduler configuration\n",
        "    # You can pass a learning scheduler such as torch.optim.lr_scheduler.StepLR\n",
        "    # kwargs: keyword arguments to torch.optim.AdamW or whatever scheduler you use\n",
        "    #\n",
        "    # Additional schedulers can be found here:\n",
        "    # https://pytorch.org/docs/stable/optim.html\n",
        "    \"lr_scheduler\": {\n",
        "        \"fn\": \"torch.optim.lr_scheduler.ExponentialLR\",\n",
        "        \"kwargs\": {\"gamma\": 1}\n",
        "    },\n",
        "\n",
        "    # Loss function configuration\n",
        "    # We use cross entropy loss\n",
        "    # kwargs: keyword arguments to torch.nn.CrossEntropyLoss or whatever loss function you use\n",
        "    \"loss\": {\n",
        "        \"fn\": \"torch.nn.CrossEntropyLoss\",\n",
        "        \"kwargs\": {}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert args dictionary to argparse namespace\n",
        "args = argparse.Namespace()\n",
        "args.__dict__ = args_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training the model\n",
        "train(args)"
      ],
      "metadata": {
        "id": "4mbzaaPOaTUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWjHpGKn5doL"
      },
      "outputs": [],
      "source": [
        "\n",
        "################ To Test the Model ####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FXKqHYMw2Ym"
      },
      "outputs": [],
      "source": [
        "# Remove existing ArabicNER model and clone the model huggingface repo\n",
        "!rm -rf /content/outputval\n",
        "#!git clone --branch flat https://huggingface.co/SinaLab/ArabicNER-Wojood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw6Mea3fxOIm"
      },
      "outputs": [],
      "source": [
        "# Import test function\n",
        "from arabiner.bin.eval import main as eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azH3HmAJk-pt"
      },
      "source": [
        "TEST on the Validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxpFMV5ExdNv"
      },
      "outputs": [],
      "source": [
        "# Setup the evaluation arguments\n",
        "args_dict = {\n",
        "    # Output path to save logs, metrics and predictions\n",
        "    \"output_path\": \"/content/outputval/\",\n",
        "\n",
        "    # train/test/validation data paths\n",
        "    # The data provided in the ArabicNER repo is a sample data\n",
        "    # data_paths takes a list of data paths in case you need to evaluate multiple datasets\n",
        "    \"data_paths\": [\"/content/ANERvalC10.txt\"],\n",
        "\n",
        "    # Path to the model, this corresponds to the \"output_path\" you specified\n",
        "    # during training the model\n",
        "    \"model_path\": \"/content/output/\",\n",
        "\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "# Convert args dictionary to argparse namespace\n",
        "args = argparse.Namespace()\n",
        "args.__dict__ = args_dict\n",
        "eval(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test on the test data"
      ],
      "metadata": {
        "id": "QFKTPXMx8q_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the evaluation arguments\n",
        "args_dict = {\n",
        "    # Output path to save logs, metrics and predictions\n",
        "    \"output_path\": \"/content/outputtest/\",\n",
        "\n",
        "    # train/test/validation data paths\n",
        "    # The data provided in the ArabicNER repo is a sample data\n",
        "    # data_paths takes a list of data paths in case you need to evaluate multiple datasets\n",
        "    \"data_paths\": [\"/content/ANERtestC10.txt\"],\n",
        "\n",
        "    # Path to the model, this corresponds to the \"output_path\" you specified\n",
        "    # during training the model\n",
        "    \"model_path\": \"/content/output/\",\n",
        "\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "# Convert args dictionary to argparse namespace\n",
        "args = argparse.Namespace()\n",
        "args.__dict__ = args_dict\n",
        "\n",
        "eval(args)"
      ],
      "metadata": {
        "id": "0HPBKUYL8qY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9uSehpJV5vG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}