{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsdz6ufUXS1l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IbOZ4BPtr4Y",
        "outputId": "5c2b016a-f810-41a0-9120-ee94ff5aa526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ ÙˆØ§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ ÙˆØ§Ù„ØµÙ„Ø§Ø© ÙˆØ§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„Ù‰ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…\n"
          ]
        }
      ],
      "source": [
        "print (\"Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ ÙˆØ§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ ÙˆØ§Ù„ØµÙ„Ø§Ø© ÙˆØ§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„Ù‰ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "tVURcr9SSOql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PEGH-Gv6WAy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#McNemar Test For Majority Voting"
      ],
      "metadata": {
        "id": "oPLM4s7PTGis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_predictions(file_path):\n",
        "    \"\"\"\n",
        "    Reads a .txt file with fixed headers:\n",
        "    Token, Gold Tag, Predicted Tag\n",
        "    Handles malformed lines safely.\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ“‚ Loading: {file_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            file_path,\n",
        "            sep=\"\\t\",\n",
        "            comment=\"#\",\n",
        "            skip_blank_lines=True,\n",
        "            engine=\"python\",\n",
        "            on_bad_lines=\"warn\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"âŒ Error reading {file_path}: {e}\")\n",
        "\n",
        "    expected_cols = [\"Token\", \"Gold Tag\", \"Predicted Tag\"]\n",
        "    if len(df.columns) > 3:\n",
        "        print(f\"âš ï¸ {file_path}: found {len(df.columns)} columns, trimming to 3.\")\n",
        "        df = df.iloc[:, :3]\n",
        "        df.columns = expected_cols\n",
        "    elif len(df.columns) == 3:\n",
        "        df.columns = expected_cols\n",
        "    else:\n",
        "        raise ValueError(f\"âŒ {file_path} has invalid column structure ({len(df.columns)} columns).\")\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna()\n",
        "    after = len(df)\n",
        "    if before != after:\n",
        "        print(f\"âš ï¸ Dropped {before - after} rows with missing values in {file_path}.\")\n",
        "    print(f\"âœ… Loaded {len(df)} valid rows from {file_path}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==== Ø§Ù„Ù…Ù„ÙØ§Øª ====\n",
        "baseline_files = [\n",
        "    \"/content/predictions_ANERtestC10_Camel-ca-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Camel-msa-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Camel-mix-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Multilingual2.txt\"\n",
        "]\n",
        "ensemble_file = \"/content/Compined_Predictions_Majority_ANER.txt\"\n",
        "\n",
        "\n",
        "# ==== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ====\n",
        "ensemble = load_predictions(ensemble_file)\n",
        "\n",
        "results_summary = []\n",
        "tables = {}\n",
        "\n",
        "# ==== Ø§Ø®ØªØ¨Ø§Ø± McNemar ====\n",
        "for file in baseline_files:\n",
        "    baseline = load_predictions(file)\n",
        "\n",
        "    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Token Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚\n",
        "    merged = pd.merge(\n",
        "        baseline,\n",
        "        ensemble,\n",
        "        on=\"Token\",\n",
        "        suffixes=(\"_base\", \"_ens\"),\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ”„ Merged {len(merged)} rows between {file.split('/')[-1]} and ensemble\")\n",
        "\n",
        "    baseline_correct = merged[\"Gold Tag_base\"] == merged[\"Predicted Tag_base\"]\n",
        "    ensemble_correct = merged[\"Gold Tag_ens\"] == merged[\"Predicted Tag_ens\"]\n",
        "\n",
        "    a = sum(baseline_correct & ensemble_correct)\n",
        "    b = sum(baseline_correct & ~ensemble_correct)\n",
        "    c = sum(~baseline_correct & ensemble_correct)\n",
        "    d = sum(~baseline_correct & ~ensemble_correct)\n",
        "\n",
        "    table = [[a, b], [c, d]]\n",
        "    tables[file] = table\n",
        "\n",
        "    result = mcnemar(table, exact=False, correction=True)\n",
        "    significance = \"Significant (p < 0.05)\" if result.pvalue < 0.05 else \"Not significant\"\n",
        "\n",
        "    results_summary.append([\n",
        "        file.replace(\".txt\", \"\").split(\"/\")[-1],\n",
        "        result.statistic,\n",
        "        result.pvalue,\n",
        "        significance\n",
        "    ])\n",
        "\n",
        "# ==== Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ====\n",
        "print(\"\\n=== Statistical Significance Results (McNemar Test) ===\\n\")\n",
        "headers = [\"Baseline Model\", \"ChiÂ² Statistic\", \"p-value\", \"Significance\"]\n",
        "print(tabulate(results_summary, headers=headers, tablefmt=\"grid\", floatfmt=\".5f\"))\n",
        "\n",
        "print(\"\\n=== Detailed 2x2 Comparison Tables ===\")\n",
        "for file, table in tables.items():\n",
        "    print(f\"\\n{file.replace('.txt', '').split('/')[-1]}:\")\n",
        "    print(tabulate(\n",
        "        table,\n",
        "        headers=[\"Ensemble Correct\", \"Ensemble Wrong\"],\n",
        "        showindex=[\"Baseline Correct\", \"Baseline Wrong\"],\n",
        "        tablefmt=\"fancy_grid\"\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "ljq8-H5g_HEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea805d5-ce03-4ba3-dbce-bab56aed8a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“‚ Loading: /content/Compined_Predictions_Majority_ANER.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/Compined_Predictions_Majority_ANER.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/Compined_Predictions_Majority_ANER.txt\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-ca-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "ğŸ”„ Merged 685599 rows between predictions_ANERtestC10_Camel-ca-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-msa-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "ğŸ”„ Merged 685608 rows between predictions_ANERtestC10_Camel-msa-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-mix-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "ğŸ”„ Merged 685607 rows between predictions_ANERtestC10_Camel-mix-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Multilingual2.txt.\n",
            "âœ… Loaded 14233 valid rows from /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "ğŸ”„ Merged 684641 rows between predictions_ANERtestC10_Multilingual2.txt and ensemble\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4068105385.py:13: ParserWarning: Skipping line 4746: Expected 3 fields in line 4746, saw 4\n",
            "\n",
            "  df = pd.read_csv(\n",
            "/tmp/ipython-input-4068105385.py:13: ParserWarning: Skipping line 13249: Expected 3 fields in line 13249, saw 5\n",
            "\n",
            "  df = pd.read_csv(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Statistical Significance Results (McNemar Test) ===\n",
            "\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "| Baseline Model                        |   ChiÂ² Statistic |   p-value | Significance           |\n",
            "+=======================================+==================+===========+========================+\n",
            "| predictions_ANERtestC10_Camel-ca-ner  |         31.36149 |   0.00000 | Significant (p < 0.05) |\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Camel-msa-ner |          8.31635 |   0.00393 | Significant (p < 0.05) |\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Camel-mix-ner |          1.07535 |   0.29974 | Not significant        |\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Multilingual2 |          1.42730 |   0.23221 | Not significant        |\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "\n",
            "=== Detailed 2x2 Comparison Tables ===\n",
            "\n",
            "predictions_ANERtestC10_Camel-ca-ner:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                  â”‚   Ensemble Correct â”‚   Ensemble Wrong â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Baseline Correct â”‚             684180 â”‚              555 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Baseline Wrong   â”‚                759 â”‚              105 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "\n",
            "predictions_ANERtestC10_Camel-msa-ner:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                  â”‚   Ensemble Correct â”‚   Ensemble Wrong â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Baseline Correct â”‚             684381 â”‚              473 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Baseline Wrong   â”‚                567 â”‚              187 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "\n",
            "predictions_ANERtestC10_Camel-mix-ner:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                  â”‚   Ensemble Correct â”‚   Ensemble Wrong â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Baseline Correct â”‚             684392 â”‚              520 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Baseline Wrong   â”‚                555 â”‚              140 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "\n",
            "predictions_ANERtestC10_Multilingual2:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                  â”‚   Ensemble Correct â”‚   Ensemble Wrong â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Baseline Correct â”‚             683400 â”‚              540 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Baseline Wrong   â”‚                581 â”‚              120 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IipTCZzvUK9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#McNemar Test Weighted Voting\n"
      ],
      "metadata": {
        "id": "DSdkFpTVVM8A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UlbaH4PuXcmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_predictions(file_path):\n",
        "    \"\"\"\n",
        "    Reads a .txt file with fixed headers:\n",
        "    Token, Gold Tag, Predicted Tag\n",
        "    Handles malformed lines safely.\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ“‚ Loading: {file_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            file_path,\n",
        "            sep=\"\\t\",\n",
        "            comment=\"#\",\n",
        "            skip_blank_lines=True,\n",
        "            engine=\"python\",\n",
        "            on_bad_lines=\"warn\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"âŒ Error reading {file_path}: {e}\")\n",
        "\n",
        "    expected_cols = [\"Token\", \"Gold Tag\", \"Predicted Tag\"]\n",
        "    if len(df.columns) > 3:\n",
        "        print(f\"âš ï¸ {file_path}: found {len(df.columns)} columns, trimming to 3.\")\n",
        "        df = df.iloc[:, :3]\n",
        "        df.columns = expected_cols\n",
        "    elif len(df.columns) == 3:\n",
        "        df.columns = expected_cols\n",
        "    else:\n",
        "        raise ValueError(f\"âŒ {file_path} has invalid column structure ({len(df.columns)} columns).\")\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna()\n",
        "    after = len(df)\n",
        "    if before != after:\n",
        "        print(f\"âš ï¸ Dropped {before - after} rows with missing values in {file_path}.\")\n",
        "    print(f\"âœ… Loaded {len(df)} valid rows from {file_path}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==== Ø§Ù„Ù…Ù„ÙØ§Øª ====\n",
        "baseline_files = [\n",
        "    \"/content/predictions_ANERtestC10_Camel-ca-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Camel-msa-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Camel-mix-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Multilingual2.txt\"\n",
        "]\n",
        "ensemble_file = \"/content/Compined_Predictions_Weighted_ANER.txt\"\n",
        "\n",
        "\n",
        "# ==== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ====\n",
        "ensemble = load_predictions(ensemble_file)\n",
        "\n",
        "results_summary = []\n",
        "tables = {}\n",
        "\n",
        "# ==== Ø§Ø®ØªØ¨Ø§Ø± McNemar ====\n",
        "for file in baseline_files:\n",
        "    baseline = load_predictions(file)\n",
        "\n",
        "    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Token Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚\n",
        "    merged = pd.merge(\n",
        "        baseline,\n",
        "        ensemble,\n",
        "        on=\"Token\",\n",
        "        suffixes=(\"_base\", \"_ens\"),\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ”„ Merged {len(merged)} rows between {file.split('/')[-1]} and ensemble\")\n",
        "\n",
        "    baseline_correct = merged[\"Gold Tag_base\"] == merged[\"Predicted Tag_base\"]\n",
        "    ensemble_correct = merged[\"Gold Tag_ens\"] == merged[\"Predicted Tag_ens\"]\n",
        "\n",
        "    a = sum(baseline_correct & ensemble_correct)\n",
        "    b = sum(baseline_correct & ~ensemble_correct)\n",
        "    c = sum(~baseline_correct & ensemble_correct)\n",
        "    d = sum(~baseline_correct & ~ensemble_correct)\n",
        "\n",
        "    table = [[a, b], [c, d]]\n",
        "    tables[file] = table\n",
        "\n",
        "    result = mcnemar(table, exact=False, correction=True)\n",
        "    significance = \"Significant (p < 0.05)\" if result.pvalue < 0.05 else \"Not significant\"\n",
        "\n",
        "    results_summary.append([\n",
        "        file.replace(\".txt\", \"\").split(\"/\")[-1],\n",
        "        result.statistic,\n",
        "        result.pvalue,\n",
        "        significance\n",
        "    ])\n",
        "\n",
        "# ==== Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ====\n",
        "print(\"\\n=== Statistical Significance Results (McNemar Test) ===\\n\")\n",
        "headers = [\"Baseline Model\", \"ChiÂ² Statistic\", \"p-value\", \"Significance\"]\n",
        "print(tabulate(results_summary, headers=headers, tablefmt=\"grid\", floatfmt=\".5f\"))\n",
        "\n",
        "print(\"\\n=== Detailed 2x2 Comparison Tables ===\")\n",
        "for file, table in tables.items():\n",
        "    print(f\"\\n{file.replace('.txt', '').split('/')[-1]}:\")\n",
        "    print(tabulate(\n",
        "        table,\n",
        "        headers=[\"Ensemble Correct\", \"Ensemble Wrong\"],\n",
        "        showindex=[\"Baseline Correct\", \"Baseline Wrong\"],\n",
        "        tablefmt=\"fancy_grid\"\n",
        "    ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgAbp5L2VQgO",
        "outputId": "ecb73b85-34ed-4f65-d011-a04544bd5f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“‚ Loading: /content/Compined_Predictions_Weighted_ANER.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/Compined_Predictions_Weighted_ANER.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/Compined_Predictions_Weighted_ANER.txt\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-ca-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "ğŸ”„ Merged 685599 rows between predictions_ANERtestC10_Camel-ca-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-msa-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "ğŸ”„ Merged 685607 rows between predictions_ANERtestC10_Camel-msa-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-mix-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "ğŸ”„ Merged 685608 rows between predictions_ANERtestC10_Camel-mix-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Multilingual2.txt.\n",
            "âœ… Loaded 14233 valid rows from /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "ğŸ”„ Merged 684642 rows between predictions_ANERtestC10_Multilingual2.txt and ensemble\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3403134850.py:13: ParserWarning: Skipping line 4746: Expected 3 fields in line 4746, saw 4\n",
            "\n",
            "  df = pd.read_csv(\n",
            "/tmp/ipython-input-3403134850.py:13: ParserWarning: Skipping line 13249: Expected 3 fields in line 13249, saw 5\n",
            "\n",
            "  df = pd.read_csv(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Statistical Significance Results (McNemar Test) ===\n",
            "\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "| Baseline Model                        |   ChiÂ² Statistic |   p-value | Significance           |\n",
            "+=======================================+==================+===========+========================+\n",
            "| predictions_ANERtestC10_Camel-ca-ner  |         33.14188 |   0.00000 | Significant (p < 0.05) |\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Camel-msa-ner |          9.38793 |   0.00218 | Significant (p < 0.05) |\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Camel-mix-ner |          1.49393 |   0.22161 | Not significant        |\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Multilingual2 |          1.89776 |   0.16833 | Not significant        |\n",
            "+---------------------------------------+------------------+-----------+------------------------+\n",
            "\n",
            "=== Detailed 2x2 Comparison Tables ===\n",
            "\n",
            "predictions_ANERtestC10_Camel-ca-ner:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                  â”‚   Ensemble Correct â”‚   Ensemble Wrong â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Baseline Correct â”‚             684181 â”‚              554 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Baseline Wrong   â”‚                764 â”‚              100 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "\n",
            "predictions_ANERtestC10_Camel-msa-ner:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                  â”‚   Ensemble Correct â”‚   Ensemble Wrong â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Baseline Correct â”‚             684381 â”‚              472 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Baseline Wrong   â”‚                572 â”‚              182 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "\n",
            "predictions_ANERtestC10_Camel-mix-ner:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                  â”‚   Ensemble Correct â”‚   Ensemble Wrong â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Baseline Correct â”‚             684398 â”‚              515 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Baseline Wrong   â”‚                556 â”‚              139 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "\n",
            "predictions_ANERtestC10_Multilingual2:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚                  â”‚   Ensemble Correct â”‚   Ensemble Wrong â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Baseline Correct â”‚             683407 â”‚              534 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Baseline Wrong   â”‚                581 â”‚              120 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Boot Strap for Majority voting"
      ],
      "metadata": {
        "id": "6sk4xRE7PY5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_predictions(file_path):\n",
        "    \"\"\"\n",
        "    Reads a .txt file with fixed headers:\n",
        "    Token, Gold Tag, Predicted Tag\n",
        "    Handles malformed lines safely.\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ“‚ Loading: {file_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            file_path,\n",
        "            sep=\"\\t\",\n",
        "            comment=\"#\",\n",
        "            skip_blank_lines=True,\n",
        "            engine=\"python\",\n",
        "            on_bad_lines=\"warn\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"âŒ Error reading {file_path}: {e}\")\n",
        "\n",
        "    expected_cols = [\"Token\", \"Gold Tag\", \"Predicted Tag\"]\n",
        "    if len(df.columns) > 3:\n",
        "        print(f\"âš ï¸ {file_path}: found {len(df.columns)} columns, trimming to 3.\")\n",
        "        df = df.iloc[:, :3]\n",
        "        df.columns = expected_cols\n",
        "    elif len(df.columns) == 3:\n",
        "        df.columns = expected_cols\n",
        "    else:\n",
        "        raise ValueError(f\"âŒ {file_path} has invalid column structure ({len(df.columns)} columns).\")\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna()\n",
        "    after = len(df)\n",
        "    if before != after:\n",
        "        print(f\"âš ï¸ Dropped {before - after} rows with missing values in {file_path}.\")\n",
        "    print(f\"âœ… Loaded {len(df)} valid rows from {file_path}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==== Ø§Ù„Ù…Ù„ÙØ§Øª ====\n",
        "baseline_files = [\n",
        "    \"/content/predictions_ANERtestC10_Camel-ca-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Camel-msa-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Camel-mix-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Multilingual2.txt\"\n",
        "]\n",
        "ensemble_file = \"/content/Compined_Predictions_Majority_ANER.txt\"\n",
        "\n",
        "\n",
        "# ==== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ====\n",
        "ensemble = load_predictions(ensemble_file)\n",
        "\n",
        "\n",
        "def bootstrap_test(baseline_correct, ensemble_correct, n_bootstrap=1000, seed=42):\n",
        "    \"\"\"Bootstrap significance test on accuracy difference.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(baseline_correct)\n",
        "    diffs = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        acc_base = np.mean(baseline_correct[idx])\n",
        "        acc_ens = np.mean(ensemble_correct[idx])\n",
        "        diffs.append(acc_ens - acc_base)\n",
        "    diffs = np.array(diffs)\n",
        "    ci_low, ci_high = np.percentile(diffs, [2.5, 97.5])\n",
        "    p_value = 2 * min(np.mean(diffs <= 0), np.mean(diffs >= 0))  # two-sided\n",
        "    return np.mean(diffs), ci_low, ci_high, p_value\n",
        "\n",
        "\n",
        "# ==== Ø§Ø®ØªØ¨Ø§Ø± Bootstrap ====\n",
        "results_summary = []\n",
        "\n",
        "for file in baseline_files:\n",
        "    baseline = load_predictions(file)\n",
        "\n",
        "    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Token Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚\n",
        "    merged = pd.merge(\n",
        "        baseline,\n",
        "        ensemble,\n",
        "        on=\"Token\",\n",
        "        suffixes=(\"_base\", \"_ens\"),\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ”„ Merged {len(merged)} rows between {file.split('/')[-1]} and ensemble\")\n",
        "\n",
        "    baseline_correct = (merged[\"Gold Tag_base\"] == merged[\"Predicted Tag_base\"]).to_numpy()\n",
        "    ensemble_correct = (merged[\"Gold Tag_ens\"] == merged[\"Predicted Tag_ens\"]).to_numpy()\n",
        "\n",
        "    mean_diff, ci_low, ci_high, p_value = bootstrap_test(baseline_correct, ensemble_correct)\n",
        "\n",
        "    significance = \"Significant (p < 0.05)\" if p_value < 0.05 else \"Not significant\"\n",
        "\n",
        "    results_summary.append([\n",
        "        file.replace(\".txt\", \"\").split(\"/\")[-1],\n",
        "        f\"{mean_diff:.5f}\",\n",
        "        f\"[{ci_low:.5f}, {ci_high:.5f}]\",\n",
        "        f\"{p_value:.5f}\",\n",
        "        significance\n",
        "    ])\n",
        "\n",
        "\n",
        "# ==== Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ====\n",
        "print(\"\\n=== Bootstrap Significance Test Results ===\\n\")\n",
        "headers = [\"Baseline Model\", \"Mean Î” Accuracy\", \"95% CI\", \"p-value\", \"Significance\"]\n",
        "print(tabulate(results_summary, headers=headers, tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FD3hjkIVZw2",
        "outputId": "9dd61047-31bd-4caf-bb0a-ae1db8bb7035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“‚ Loading: /content/Compined_Predictions_Majority_ANER.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/Compined_Predictions_Majority_ANER.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/Compined_Predictions_Majority_ANER.txt\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-ca-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "ğŸ”„ Merged 685599 rows between predictions_ANERtestC10_Camel-ca-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-msa-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "ğŸ”„ Merged 685608 rows between predictions_ANERtestC10_Camel-msa-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-mix-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "ğŸ”„ Merged 685607 rows between predictions_ANERtestC10_Camel-mix-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Multilingual2.txt.\n",
            "âœ… Loaded 14233 valid rows from /content/predictions_ANERtestC10_Multilingual2.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2554979189.py:13: ParserWarning: Skipping line 4746: Expected 3 fields in line 4746, saw 4\n",
            "\n",
            "  df = pd.read_csv(\n",
            "/tmp/ipython-input-2554979189.py:13: ParserWarning: Skipping line 13249: Expected 3 fields in line 13249, saw 5\n",
            "\n",
            "  df = pd.read_csv(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Merged 684641 rows between predictions_ANERtestC10_Multilingual2.txt and ensemble\n",
            "\n",
            "=== Bootstrap Significance Test Results ===\n",
            "\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n",
            "| Baseline Model                        |   Mean Î” Accuracy | 95% CI              |   p-value | Significance           |\n",
            "+=======================================+===================+=====================+===========+========================+\n",
            "| predictions_ANERtestC10_Camel-ca-ner  |           0.0003  | [0.00019, 0.00040]  |     0     | Significant (p < 0.05) |\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Camel-msa-ner |           0.00013 | [0.00004, 0.00022]  |     0.008 | Significant (p < 0.05) |\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Camel-mix-ner |           5e-05   | [-0.00005, 0.00014] |     0.324 | Not significant        |\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Multilingual2 |           6e-05   | [-0.00003, 0.00015] |     0.198 | Not significant        |\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Boot Strap for weighted voting"
      ],
      "metadata": {
        "id": "vH9ker1YXobB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_predictions(file_path):\n",
        "    \"\"\"\n",
        "    Reads a .txt file with fixed headers:\n",
        "    Token, Gold Tag, Predicted Tag\n",
        "    Handles malformed lines safely.\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ“‚ Loading: {file_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            file_path,\n",
        "            sep=\"\\t\",\n",
        "            comment=\"#\",\n",
        "            skip_blank_lines=True,\n",
        "            engine=\"python\",\n",
        "            on_bad_lines=\"warn\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"âŒ Error reading {file_path}: {e}\")\n",
        "\n",
        "    expected_cols = [\"Token\", \"Gold Tag\", \"Predicted Tag\"]\n",
        "    if len(df.columns) > 3:\n",
        "        print(f\"âš ï¸ {file_path}: found {len(df.columns)} columns, trimming to 3.\")\n",
        "        df = df.iloc[:, :3]\n",
        "        df.columns = expected_cols\n",
        "    elif len(df.columns) == 3:\n",
        "        df.columns = expected_cols\n",
        "    else:\n",
        "        raise ValueError(f\"âŒ {file_path} has invalid column structure ({len(df.columns)} columns).\")\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna()\n",
        "    after = len(df)\n",
        "    if before != after:\n",
        "        print(f\"âš ï¸ Dropped {before - after} rows with missing values in {file_path}.\")\n",
        "    print(f\"âœ… Loaded {len(df)} valid rows from {file_path}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==== Ø§Ù„Ù…Ù„ÙØ§Øª ====\n",
        "baseline_files = [\n",
        "    \"/content/predictions_ANERtestC10_Camel-ca-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Camel-msa-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Camel-mix-ner.txt\",\n",
        "    \"/content/predictions_ANERtestC10_Multilingual2.txt\"\n",
        "]\n",
        "ensemble_file = \"/content/Compined_Predictions_Weighted_ANER.txt\"\n",
        "\n",
        "\n",
        "# ==== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ====\n",
        "ensemble = load_predictions(ensemble_file)\n",
        "\n",
        "\n",
        "def bootstrap_test(baseline_correct, ensemble_correct, n_bootstrap=1000, seed=42):\n",
        "    \"\"\"Bootstrap significance test on accuracy difference.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(baseline_correct)\n",
        "    diffs = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        acc_base = np.mean(baseline_correct[idx])\n",
        "        acc_ens = np.mean(ensemble_correct[idx])\n",
        "        diffs.append(acc_ens - acc_base)\n",
        "    diffs = np.array(diffs)\n",
        "    ci_low, ci_high = np.percentile(diffs, [2.5, 97.5])\n",
        "    p_value = 2 * min(np.mean(diffs <= 0), np.mean(diffs >= 0))  # two-sided\n",
        "    return np.mean(diffs), ci_low, ci_high, p_value\n",
        "\n",
        "\n",
        "# ==== Ø§Ø®ØªØ¨Ø§Ø± Bootstrap ====\n",
        "results_summary = []\n",
        "\n",
        "for file in baseline_files:\n",
        "    baseline = load_predictions(file)\n",
        "\n",
        "    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Token Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚\n",
        "    merged = pd.merge(\n",
        "        baseline,\n",
        "        ensemble,\n",
        "        on=\"Token\",\n",
        "        suffixes=(\"_base\", \"_ens\"),\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ”„ Merged {len(merged)} rows between {file.split('/')[-1]} and ensemble\")\n",
        "\n",
        "    baseline_correct = (merged[\"Gold Tag_base\"] == merged[\"Predicted Tag_base\"]).to_numpy()\n",
        "    ensemble_correct = (merged[\"Gold Tag_ens\"] == merged[\"Predicted Tag_ens\"]).to_numpy()\n",
        "\n",
        "    mean_diff, ci_low, ci_high, p_value = bootstrap_test(baseline_correct, ensemble_correct)\n",
        "\n",
        "    significance = \"Significant (p < 0.05)\" if p_value < 0.05 else \"Not significant\"\n",
        "\n",
        "    results_summary.append([\n",
        "        file.replace(\".txt\", \"\").split(\"/\")[-1],\n",
        "        f\"{mean_diff:.5f}\",\n",
        "        f\"[{ci_low:.5f}, {ci_high:.5f}]\",\n",
        "        f\"{p_value:.5f}\",\n",
        "        significance\n",
        "    ])\n",
        "\n",
        "\n",
        "# ==== Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ====\n",
        "print(\"\\n=== Bootstrap Significance Test Results ===\\n\")\n",
        "headers = [\"Baseline Model\", \"Mean Î” Accuracy\", \"95% CI\", \"p-value\", \"Significance\"]\n",
        "print(tabulate(results_summary, headers=headers, tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEtnWzxhSJPv",
        "outputId": "7a9a8ea3-f514-40bd-b11c-86318cb203f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“‚ Loading: /content/Compined_Predictions_Weighted_ANER.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/Compined_Predictions_Weighted_ANER.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/Compined_Predictions_Weighted_ANER.txt\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-ca-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "ğŸ”„ Merged 685599 rows between predictions_ANERtestC10_Camel-ca-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-msa-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "ğŸ”„ Merged 685607 rows between predictions_ANERtestC10_Camel-msa-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Camel-mix-ner.txt.\n",
            "âœ… Loaded 14235 valid rows from /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "ğŸ”„ Merged 685608 rows between predictions_ANERtestC10_Camel-mix-ner.txt and ensemble\n",
            "\n",
            "ğŸ“‚ Loading: /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "âš ï¸ Dropped 2 rows with missing values in /content/predictions_ANERtestC10_Multilingual2.txt.\n",
            "âœ… Loaded 14233 valid rows from /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "ğŸ”„ Merged 684642 rows between predictions_ANERtestC10_Multilingual2.txt and ensemble\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1264013803.py:13: ParserWarning: Skipping line 4746: Expected 3 fields in line 4746, saw 4\n",
            "\n",
            "  df = pd.read_csv(\n",
            "/tmp/ipython-input-1264013803.py:13: ParserWarning: Skipping line 13249: Expected 3 fields in line 13249, saw 5\n",
            "\n",
            "  df = pd.read_csv(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Bootstrap Significance Test Results ===\n",
            "\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n",
            "| Baseline Model                        |   Mean Î” Accuracy | 95% CI              |   p-value | Significance           |\n",
            "+=======================================+===================+=====================+===========+========================+\n",
            "| predictions_ANERtestC10_Camel-ca-ner  |           0.00031 | [0.00020, 0.00041]  |     0     | Significant (p < 0.05) |\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Camel-msa-ner |           0.00014 | [0.00006, 0.00024]  |     0.006 | Significant (p < 0.05) |\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Camel-mix-ner |           6e-05   | [-0.00004, 0.00015] |     0.24  | Not significant        |\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n",
            "| predictions_ANERtestC10_Multilingual2 |           7e-05   | [-0.00002, 0.00017] |     0.148 | Not significant        |\n",
            "+---------------------------------------+-------------------+---------------------+-----------+------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cP3J3X3wSQu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}